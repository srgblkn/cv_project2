from __future__ import annotations

import base64
import io
import zipfile
from pathlib import Path
import importlib.util

import numpy as np
import pandas as pd
import streamlit as st
from PIL import Image

# Torch –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ª–µ–Ω–∏–≤–æ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ), —á—Ç–æ–±—ã —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –ª–æ–≥–∞–º–∏ –Ω–µ –ø–∞–¥–∞–ª–∞
# –µ—Å–ª–∏ –≤–¥—Ä—É–≥ torch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω/–∫–æ–Ω—Ñ–ª–∏–∫—Ç –≤–µ—Ä—Å–∏–π.


# -----------------------------
# Paths (—Å—Ç—Ä–æ–≥–æ)
# -----------------------------
THIS_DIR = Path(__file__).resolve().parent
ART_DIR = THIS_DIR / "forrest"

MODEL_PY = ART_DIR / "model_class.py"
LOG_CSV = ART_DIR / "unet_training_log.csv"
BG_JPG = ART_DIR / "forrest.jpg"


# -----------------------------
# Page config
# -----------------------------
st.set_page_config(page_title="Forest Segmentation", page_icon="üå≤", layout="wide")


# -----------------------------
# UI: background + opaque cards
# -----------------------------
def apply_background(bg_path: Path) -> None:
    bg_css = ""
    if bg_path.exists():
        b64 = base64.b64encode(bg_path.read_bytes()).decode("utf-8")
        bg_css = f"""
        .stApp {{
            background-image: url("data:image/jpeg;base64,{b64}");
            background-size: cover;
            background-position: center;
            background-attachment: fixed;
        }}
        """

    st.markdown(
        f"""
        <style>
        {bg_css}

        .stApp, .stMarkdown, .stText, .stCaption, .stWrite {{
            color: #F8FAFC;
        }}

        header[data-testid="stHeader"] {{ background: rgba(0,0,0,0); }}

        section[data-testid="stSidebar"] {{
            background: #0B1220;
            border-right: 1px solid rgba(255,255,255,0.10);
        }}
        section[data-testid="stSidebar"] * {{ color: #F8FAFC !important; }}

        .opaque-card {{
            background: #0B1220;
            border: 1px solid rgba(255,255,255,0.12);
            border-radius: 18px;
            padding: 16px 16px 14px 16px;
            box-shadow: 0 10px 24px rgba(0,0,0,0.40);
            margin-bottom: 14px;
        }}
        .opaque-card h3 {{
            margin: 0;
            font-size: 1.25rem;
            font-weight: 750;
            color: #F8FAFC;
        }}
        .opaque-card p {{
            margin: 6px 0 0 0;
            color: rgba(248,250,252,0.85);
            line-height: 1.35;
        }}

        div[data-testid="stExpander"] > details {{
            background: #0B1220;
            border: 1px solid rgba(255,255,255,0.12);
            border-radius: 18px;
            padding: 10px 12px;
            box-shadow: 0 10px 24px rgba(0,0,0,0.30);
        }}
        div[data-testid="stExpander"] summary {{
            color: #F8FAFC !important;
            font-weight: 650;
        }}

        div[data-testid="stFileUploader"] section {{
            background: #0B1220;
            border: 1px solid rgba(255,255,255,0.12);
            border-radius: 18px;
            padding: 10px;
        }}

        .stButton > button {{
            border-radius: 14px;
            border: 1px solid rgba(255,255,255,0.14);
        }}

        a {{ color: #93C5FD !important; }}
        </style>
        """,
        unsafe_allow_html=True,
    )


def opaque_card(title: str, text: str) -> None:
    st.markdown(
        f"""
        <div class="opaque-card">
          <h3>{title}</h3>
          <p>{text}</p>
        </div>
        """,
        unsafe_allow_html=True,
    )


def safe_switch_page(target: str) -> None:
    if hasattr(st, "switch_page"):
        try:
            st.switch_page(target)
        except Exception:
            st.info("–ü–µ—Ä–µ—Ö–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—é —Å–ª–µ–≤–∞.")
    else:
        st.info("–ü–µ—Ä–µ—Ö–æ–¥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—é —Å–ª–µ–≤–∞.")


apply_background(BG_JPG)


# -----------------------------
# Model import (from existing model_class.py)
# -----------------------------
def import_unet_class(model_py: Path):
    if not model_py.exists():
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏: {model_py.as_posix()}")

    spec = importlib.util.spec_from_file_location("forrest_model_class", model_py.as_posix())
    if spec is None or spec.loader is None:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å import spec –¥–ª—è model_class.py")

    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)  # –º–æ–∂–µ—Ç –±—Ä–æ—Å–∏—Ç—å ImportError (–Ω–∞–ø—Ä–∏–º–µ—Ä, torchmetrics)
    if not hasattr(module, "UNet"):
        raise AttributeError("–í model_class.py –Ω–µ –Ω–∞–π–¥–µ–Ω –∫–ª–∞—Å—Å UNet")
    return module.UNet


def find_weight_candidates(dir_path: Path) -> list[Path]:
    exts = ("*.pt", "*.pth", "*.ckpt")
    files: list[Path] = []
    for pat in exts:
        files.extend(dir_path.glob(pat))
    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: –±–æ–ª–µ–µ ‚Äú–ø–æ—Ö–æ–∂–µ–µ –Ω–∞ best‚Äù –≤–≤–µ—Ä—Ö
    files = sorted(files, key=lambda p: ("best" not in p.name.lower(), p.name.lower()))
    return files


# -----------------------------
# Image utils
# -----------------------------
def to_tensor_rgb(img: Image.Image):
    # lazy torch import
    import torch

    arr = np.array(img.convert("RGB")).astype(np.float32) / 255.0  # HWC
    t = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)  # 1CHW
    return t


def sigmoid_mask(logits):
    import torch

    probs = torch.sigmoid(logits)
    return probs


def overlay_mask_on_image(img: Image.Image, mask_2d: np.ndarray, alpha: float = 0.45) -> Image.Image:
    """
    mask_2d: float/uint8 [H,W] where 1=forest, 0=background
    """
    base = img.convert("RGBA")
    h, w = mask_2d.shape
    # –∑–µ–ª—ë–Ω–∞—è –∑–∞–ª–∏–≤–∫–∞
    overlay = Image.new("RGBA", (w, h), (0, 0, 0, 0))
    overlay_np = np.array(overlay)

    m = (mask_2d > 0.5)
    overlay_np[m] = np.array([46, 204, 113, int(255 * alpha)], dtype=np.uint8)  # green
    overlay = Image.fromarray(overlay_np, mode="RGBA")

    return Image.alpha_composite(base, overlay).convert("RGB")


def mask_to_png_bytes(mask_2d: np.ndarray) -> bytes:
    # –º–∞—Å–∫–∞ –∫–∞–∫ 0/255
    m = (mask_2d > 0.5).astype(np.uint8) * 255
    im = Image.fromarray(m, mode="L")
    buf = io.BytesIO()
    im.save(buf, format="PNG")
    return buf.getvalue()


def pil_to_png_bytes(img: Image.Image) -> bytes:
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()


# -----------------------------
# Header
# -----------------------------
opaque_card(
    "Forest Segmentation",
    "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–∞—Å–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –ø–æ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–º —Å–Ω–∏–º–∫–∞–º: –ø—Ä–µ–≤—å—é, –Ω–∞–ª–æ–∂–µ–Ω–∏–µ –∏ –≤—ã–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.",
)

h1, h2 = st.columns([1, 1], gap="large")
with h1:
    if st.button("‚Üê –ù–∞ –≥–ª–∞–≤–Ω—É—é", use_container_width=True):
        safe_switch_page("app.py")
with h2:
    if BG_JPG.exists():
        st.download_button(
            "–°–∫–∞—á–∞—Ç—å —Ñ–æ–Ω (JPG)",
            data=BG_JPG.read_bytes(),
            file_name=BG_JPG.name,
            mime="image/jpeg",
            use_container_width=True,
        )


# -----------------------------
# Sidebar: settings
# -----------------------------
st.sidebar.markdown("## –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
preset = st.sidebar.selectbox("–†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏", ["Fast", "Balanced", "High"], index=1)
size_map = {"Fast": 256, "Balanced": 384, "High": 512}
img_size = size_map[preset]

threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ –º–∞—Å–∫–∏", 0.05, 0.95, 0.50, 0.05)
alpha = st.sidebar.slider("–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å –Ω–∞–ª–æ–∂–µ–Ω–∏—è", 0.10, 0.90, 0.45, 0.05)

export_mode = st.sidebar.selectbox("–≠–∫—Å–ø–æ—Ä—Ç", ["ZIP (–º–∞—Å–∫–∏ + overlay)", "ZIP (—Ç–æ–ª—å–∫–æ –º–∞—Å–∫–∏)"], index=0)


# -----------------------------
# Training log (unet_training_log.csv): charts by request
# -----------------------------
log_df = None
if LOG_CSV.exists():
    try:
        log_df = pd.read_csv(LOG_CSV)
    except Exception:
        log_df = None

right_info, right_charts = st.columns([1.0, 1.0], gap="large")

with right_info:
    opaque_card("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è", "–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –¥–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è (–ø–æ –ª–æ–≥-—Ñ–∞–π–ª—É).")
    if log_df is None:
        st.info(f"–õ–æ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —á–∏—Ç–∞–µ—Ç—Å—è: `{LOG_CSV.as_posix()}`")
    else:
        # –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∏—â–µ–º epoch
        epoch_col = next((c for c in log_df.columns if c.lower() in ("epoch", "epochs", "step")), None)
        # –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è —Å–≤–æ–¥–∫–∞ ‚Äú–ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞‚Äù
        tail = log_df.tail(1).copy()
        st.dataframe(tail, use_container_width=True, hide_index=True)

with right_charts:
    opaque_card("–ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è", "–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
    if log_df is None:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤.")
    else:
        epoch_col = next((c for c in log_df.columns if c.lower() in ("epoch", "step")), None)
        if epoch_col is None:
            st.dataframe(log_df.tail(30), use_container_width=True)
        else:
            numeric_cols = [c for c in log_df.columns if c != epoch_col and pd.api.types.is_numeric_dtype(log_df[c])]
            if not numeric_cols:
                st.dataframe(log_df.tail(30), use_container_width=True)
            else:
                default = numeric_cols[:1]
                selected = st.multiselect("–ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏", options=numeric_cols, default=default)
                if selected:
                    chart = log_df[[epoch_col] + selected].copy().set_index(epoch_col)
                    st.line_chart(chart, use_container_width=True)


st.divider()


# -----------------------------
# Inference UI
# -----------------------------
left, right = st.columns([1.25, 1.0], gap="large")

with left:
    opaque_card("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–Ω–∏–º–∫–æ–≤", "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –æ–¥–Ω–∏–º ZIP.")
    uploads = st.file_uploader(
        "Images",
        type=["png", "jpg", "jpeg", "bmp", "tif", "tiff"],
        accept_multiple_files=True,
        label_visibility="collapsed",
    )

    if uploads:
        with st.expander("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä", expanded=True):
            cols = st.columns(4)
            for i, up in enumerate(uploads):
                try:
                    up.seek(0)
                    img = Image.open(up).convert("RGB")
                    cols[i % 4].image(img, caption=up.name, use_container_width=True)
                    up.seek(0)
                except Exception:
                    cols[i % 4].write(up.name)

    run_btn = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é", type="primary", use_container_width=True)

with right:
    opaque_card("–í–µ—Å–∞ –º–æ–¥–µ–ª–∏", "–ê–≤—Ç–æ–ø–æ–∏—Å–∫ –≤–µ—Å–æ–≤ –≤ –ø–∞–ø–∫–µ `pages/forrest/` –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Ä—É—á–Ω—É—é.")
    candidates = find_weight_candidates(ART_DIR)
    chosen_path = None

    if candidates:
        chosen_name = st.selectbox("–ù–∞–π–¥–µ–Ω–Ω—ã–µ –≤–µ—Å–∞", [p.name for p in candidates], index=0)
        chosen_path = ART_DIR / chosen_name
        st.caption(f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: `{chosen_path.as_posix()}`")
    else:
        st.warning("–§–∞–π–ª—ã –≤–µ—Å–æ–≤ (*.pt/*.pth/*.ckpt) —Ä—è–¥–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        uploaded_weights = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å–∞", type=["pt", "pth", "ckpt"], accept_multiple_files=False)
        if uploaded_weights is not None:
            # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞)
            tmp_path = Path("/tmp") / uploaded_weights.name
            tmp_path.write_bytes(uploaded_weights.getbuffer())
            chosen_path = tmp_path
            st.caption(f"–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: `{chosen_path.as_posix()}`")

    st.caption(f"–ú–æ–¥–µ–ª—å-–∫–ª–∞—Å—Å: `{MODEL_PY.as_posix()}`")
    st.caption(f"–õ–æ–≥: `{LOG_CSV.as_posix()}`")
    st.caption(f"–§–æ–Ω: `{BG_JPG.as_posix()}`")


# -----------------------------
# Run inference
# -----------------------------
@st.cache_resource(show_spinner=False)
def load_model_and_weights(model_py: str, weights_path: str):
    import torch

    UNet = import_unet_class(Path(model_py))  # –º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å –ø–æ ImportError (torchmetrics –∏ —Ç.–ø.)
    model = UNet(n_class=1)

    ckpt = torch.load(weights_path, map_location="cpu")

    # —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —á–µ–∫–ø–æ–π–Ω—Ç–æ–≤
    if isinstance(ckpt, dict) and "state_dict" in ckpt and isinstance(ckpt["state_dict"], dict):
        state = ckpt["state_dict"]
    elif isinstance(ckpt, dict) and all(isinstance(k, str) for k in ckpt.keys()):
        state = ckpt
    else:
        # –æ—á–µ–Ω—å —Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π: —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Ü–µ–ª–∏–∫–æ–º model
        state = None

    if state is not None:
        # –∏–Ω–æ–≥–¥–∞ –∫–ª—é—á–∏ –±—ã–≤–∞—é—Ç —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º "module."
        cleaned = {}
        for k, v in state.items():
            nk = k.replace("module.", "")
            cleaned[nk] = v
        model.load_state_dict(cleaned, strict=False)

    model.eval()
    return model


if run_btn:
    if not uploads:
        st.warning("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª.")
        st.stop()
    if chosen_path is None:
        st.error("–ù–µ –≤—ã–±—Ä–∞–Ω—ã –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏. –ü–æ–ª–æ–∂–∏—Ç–µ —Ñ–∞–π–ª –≤–µ—Å–æ–≤ –≤ `pages/forrest/` –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Ä—É—á–Ω—É—é.")
        st.stop()

    try:
        with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏ –≤–µ—Å–∞..."):
            model = load_model_and_weights(MODEL_PY.as_posix(), chosen_path.as_posix())
    except ImportError as e:
        st.error(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å `model_class.py` –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.\n\n"
            f"–û—à–∏–±–∫–∞: {e}\n\n"
            "–†–µ—à–µ–Ω–∏–µ: –¥–æ–±–∞–≤—å—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π –ø–∞–∫–µ—Ç –≤ `requirements.txt` (–Ω–∞–ø—Ä–∏–º–µ—Ä, `torchmetrics`)."
        )
        st.stop()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏/–≤–µ—Å–æ–≤: {e}")
        st.stop()

    import torch

    results_for_zip: list[tuple[str, bytes]]
